% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fit_lscggm.R
\name{lscggmadmm}
\alias{lscggmadmm}
\title{Fit the conditional low-rank plus sparse estimator using the alternating method direction of multipliers}
\usage{
lscggmadmm(
  SigmaZ,
  SigmaZX,
  SigmaX,
  Lambda1,
  Lambda2,
  init = NULL,
  maxiter = 1000,
  mu = 1,
  abs_tol = 1e-04,
  rel_tol = 0.01,
  print_progress = TRUE,
  print_every = 10
)
}
\arguments{
\item{SigmaZ}{An estimate of the correlation matrix of Z. m x m matrix.}

\item{SigmaZX}{An estimate of the cross-correlation matrix between Z and X. p x m matrix.}

\item{SigmaX}{An estimate of the correlation matrix of X. p x p matrix.}

\item{Lambda1}{Penalty on the l1 norm of S}

\item{Lambda2}{Penalty on the sum of the singular values of L}

\item{init}{The output of a previous run of the algorithm. For warm starts. Defaults to NULL.}

\item{maxiter}{Maximal number of iterations}

\item{mu}{Step size of the ADMM algorithm.}

\item{abs_tol}{Absolute tolerance required to stop the algorithm. Default 1e-04.}

\item{rel_tol}{Relative tolerance required to stop the algorithm. The algorithm
stops when both the change in parameters is below tolerance and the constraints
are satisfied. Default 1e-02.}

\item{print_progress}{Whether the algorithm should report on its progress.}

\item{print_every}{How often should the algorithm report on its progress (in terms
of #iterations).}
}
\value{
An S3 object of class lscggm It is essentially a list with keys:
\describe{
 \item{SX}{A p x p matrix. The sparse estimate SX.}
 \item{LX}{A p x p matrix. The low-rank estimate LX.}
 \item{SZX}{A p x p matrix. The sparse estimate SZX.}
 \item{LZX}{A p x p matrix. The low-rank estimate LZX.}
 \item{UX}{A p x p matrix. Augmented Lagrangian multiplier. It is stored in order to allow warm starts.}
 \item{AX}{A p x p matrix. Used interall. Stored to allow warm starts.}
 \item{UZX}{A p x p matrix. Augmented Lagrangian multiplier. It is stored in order to allow warm starts.}
 \item{AZX}{A p x p matrix. Used internally. Stored to allow warm starts}
 \item{termcode}{An integer. Its value determines whether the algorithm terminated normally or with an error.
 0: Convergence reached. -1: Maxiter reached. -2: Shrinkage too strong.}
 \item{termmsg}{A character vector. The message corresponding to the value \code{termcode}.}
 \item{history}{A numerical dataframe with the objective function at each
 iterations, the norm and dual norm as well the primal and dual tolerance
 criteria for convergence. The algorithm exits when r_norm < eps_pri and s_norm < eps_dual.}
}
}
\description{
The estimator fits a low-rank plus sparse estimator to a set of observed variables X
conditionally on a set of variables Z. 
Given a n x p data matrix X with empirical correlation matrix
\eqn{\Sigma_X}, and an n x m data matrix Z with empirical correlation matrix \eqn{\Sigma_Z}, 
an alternating direction method of multipliers (ADMM) algorithm is fitted to obtain the solutions \eqn{S, L} to
\deqn{(S, L) = argmin_{A, B}  -loglikelihood(A, B, ; \Sigma_Z, \Sigma_X) + \lambda_1 ||A||_1 + \lambda_2 ||B||_\ast),}
subject to \eqn{A_X - B_X > 0} and \eqn{B_X \ge 0}.
loglikelihood is the log-likelihood of a conditional Gaussian graphical model (see reference
for notations and further details).
}
\details{
Given a n x p data matrix X with empirical correlation matrix
\eqn{\Sigma_X}, and an n x m data matrix Z with empirical correlation matrix \eqn{\Sigma_Z}, 
an alternating direction method of multipliers (ADMM) algorithm is fitted to obtain the solutions \eqn{S, L} to
\deqn{(S, L) = argmin_{A, B}  -loglikelihood(A, B, ; \Sigma_Z, \Sigma_X) + \lambda_1 ||A||_1 + \lambda_2 ||B||_\ast),}
subject to \eqn{A_X - B_X > 0} and \eqn{B_X \ge 0}.
loglikelihood is the log-likelihood of a conditional Gaussian graphical model (see reference
for notations and further details).
This is the estimator suggested in Frot, Jostins and McVean.

The optimisation problem is decomposed as a three-block ADMM optimisation problem, as described in Ye et al.
Because it is a so-called consensus problem, the ADMM is guaranteed to converge.

The tuning parameters \eqn{\lambda_1} and \eqn{\lambda_2} are typically reparametrised as
\eqn{\lambda_1 = \lambda \gamma} and \eqn{\lambda_2 = \lambda (1 - \gamma)}, for \eqn{\gamma \in (0,1)}.
Here, for a fixed \eqn{\gamma}, \eqn{\lambda} controls the overall shrinkage along the path defined by \eqn{\gamma}.
\eqn{\gamma} controls the trade off on the penalties between sparse and low-rank components.

For numerical stability, a smaller value of \eqn{\gamma} is preferable when the number of samples n is close to p. 
See examples.
}
\examples{
set.seed(1)
# Generate data for a well-powered dataset
sim.data <- generate.latent.ggm.data(n=2000, p=100, h=5, outlier.fraction = 0.0,
                                     sparsity = 0.02, sparsity.latent = 0.7)
X <- sim.data$obs.data; Sigma <- cor(X) # Sample correlation matrix

}
\references{
B Frot, L Jostins, G McVean. Graphical model selection for Gaussian conditional random fields in the presence of latent variables
Journal of the American Statistical Association 114 (526), 723-734

Chandrasekaran, Venkat; Parrilo, Pablo A.; Willsky, Alan S. Latent variable graphical model selection via convex optimization.
Ann. Statist. 40 (2012), no. 4, 1935--1967. doi:10.1214/11-AOS949. \url{https://projecteuclid.org/euclid.aos/1351602527}

Gui-Bo Ye, Yuanfeng Wang, Yifei Chen, Xiaohui Xie;
Efficient Latent Variable Graphical Model Selection via Split Bregman Method.
\url{https://arxiv.org/abs/1110.3076}
}
\seealso{
lscggm.cv lscggm.path
}
